{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = '{}/research_backup/data'.format(os.getenv('HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--subqs_dir\",\n",
    "                    required=True,\n",
    "                    type=str,\n",
    "                    help=\"Directory containing UMT-Generated Sub-Qs.\")\n",
    "parser.add_argument(\"--splits\",\n",
    "                    required=True,\n",
    "                    nargs='+',\n",
    "                    help=\"Dataset splits to process (e.g., dev).\")\n",
    "parser.add_argument(\"--num_subas\",\n",
    "                    default=1,\n",
    "                    type=int,\n",
    "                    help=\"Number of Sub-answers per Sub-Q.\")\n",
    "parser.add_argument(\"--subsample_data\",\n",
    "                    action='store_true',\n",
    "                    default=False,\n",
    "                    help=\"Make subsets of train with fewer Sub-Qs?\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Load SQUAD formatted hotpot data\n",
    "hotpot_data_folder = 'hotpot-squad'\n",
    "data = {}\n",
    "for split in args.splits:\n",
    "    with open(f'{DATA_DIR}/{hotpot_data_folder}/{split}.json') as f:\n",
    "        data[split] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subqs = {}\n",
    "subas = {}\n",
    "qtype = 'c'\n",
    "total_raw_subqs = 0\n",
    "raw_subas = {}\n",
    "for split in args.splits:\n",
    "    with open(f'{args.subqs_dir}/{split}.json') as f:\n",
    "        raw_subqs = json.load(f)\n",
    "    with open(f'{args.subqs_dir}/bert_predict.nbest=10/{split}.nbest_predictions.json') as f:\n",
    "        raw_subas.update(json.load(f))\n",
    "    for example in raw_subqs['data']:\n",
    "        for paragraph in example['paragraphs']:\n",
    "            qid = paragraph['qas'][0]['id'].split('-')[0]\n",
    "            subqs[qid] = subqs.get(qid, {})\n",
    "            for qa in paragraph['qas']:\n",
    "                total_raw_subqs += 1\n",
    "                subqs[qid][qtype] = subqs[qid].get(qtype, [])\n",
    "                subqs[qid][qtype].append(qa['question'])\n",
    "\n",
    "total_subas = 0\n",
    "num_missing_subas = 0\n",
    "subas_c_qids = {k.split('-')[0] for k in raw_subas.keys()}\n",
    "for qid in subas_c_qids:\n",
    "    subas[qid] = subas.get(qid, {})\n",
    "    subas[qid][qtype] = []\n",
    "    for i in [0, 1]:\n",
    "        subqid = qid + '-' + str(i)\n",
    "        if subqid in raw_subas:\n",
    "            subas[qid][qtype].append(raw_subas[subqid])\n",
    "            total_subas += 1\n",
    "        else:\n",
    "            num_missing_subas += 1\n",
    "# suba_probs = np.array(suba_probs)\n",
    "# suba_probs = np.array(np.array(suba_probs[i]) for i in suba_probs)\n",
    "# suba_probs.sort()\n",
    "print(f'total_raw_subqs={total_raw_subqs}, total_subas={total_subas}, num_missing_subas={num_missing_subas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot histogram of sub-A model confidence\n",
    "# plt.hist(suba_probs, 100, density=True, range=(0,1))\n",
    "# plt.xlabel('Confidence in Sub-Answer')\n",
    "# plt.ylabel('% of Dev Examples')\n",
    "# plt.title('Histogram of Single-hop QA Model Confidence (Best sub-A)')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for si in range(args.num_subas):\n",
    "    for sj in range(args.num_subas):\n",
    "        suba_rank = [si, sj]\n",
    "        use_subq = True\n",
    "        use_suba = True\n",
    "        skip_qtypes = ['b', 'i', 'o']\n",
    "        limitsubqs = True\n",
    "\n",
    "        q_start = '//'\n",
    "        a_start = '/'\n",
    "        skip_qtypes.sort()\n",
    "\n",
    "        assert use_subq or use_suba, 'Adding no sub-Qs or sub-As. Are you sure?'\n",
    "        question_augmenteds = []\n",
    "        print('Copying data...')\n",
    "        data_copy = deepcopy(data)\n",
    "        for split in data_copy.keys():\n",
    "            for article_no in tqdm(range(len(data_copy[split]['data']))):\n",
    "                for paragraph_no in range(len(data_copy[split]['data'][article_no]['paragraphs'])):\n",
    "                    if '_id' not in data_copy[split]['data'][article_no]['paragraphs'][paragraph_no]:\n",
    "                        continue  # SQuAD question: skip\n",
    "                    qid = data_copy[split]['data'][article_no]['paragraphs'][paragraph_no]['_id']\n",
    "                    for qa_no in range(len(data_copy[split]['data'][article_no]['paragraphs'][paragraph_no]['qas'])):\n",
    "                        question_augmented = data_copy[split]['data'][article_no]['paragraphs'][paragraph_no]['qas'][qa_no]['question'].strip()\n",
    "                        if use_suba and ('o' not in skip_qtypes):\n",
    "                            question_augmented += ' ' + a_start + ' ' + subas[qid]['o'][0].strip()\n",
    "                        for qtype in {'b', 'i', 'c'}:\n",
    "                            if (qtype not in skip_qtypes) and (qid in subqs) and (qtype in subqs[qid]):\n",
    "                                if limitsubqs and ('c' in subqs[qid]) and (qtype in {'b', 'i'}):\n",
    "                                    print('Limited!')\n",
    "                                    continue  # don't add b/i subqs if there's a c decomposition\n",
    "                                for subq_no, (subq, suba) in enumerate(zip(subqs[qid][qtype], subas[qid][qtype])):\n",
    "                                    if use_subq:\n",
    "                                        question_augmented += ' ' + q_start + ' ' + subq.strip()\n",
    "                                    if use_suba and (len(suba) > suba_rank[subq_no]):\n",
    "                                        question_augmented += ' ' + a_start + ' ' + suba[suba_rank[subq_no]]['text'].strip()\n",
    "                                        data_copy[split]['data'][article_no]['paragraphs'][paragraph_no]['qas'][qa_no]['probability_subas'] = data_copy[split]['data'][article_no]['paragraphs'][paragraph_no]['qas'][qa_no].get('probability_subas', [])\n",
    "                                        data_copy[split]['data'][article_no]['paragraphs'][paragraph_no]['qas'][qa_no]['probability_subas'].append(suba[suba_rank[subq_no]]['probability'])\n",
    "                        data_copy[split]['data'][article_no]['paragraphs'][paragraph_no]['qas'][qa_no]['question'] = question_augmented\n",
    "                        question_augmenteds.append(question_augmented)\n",
    "                    \n",
    "        qlens = []\n",
    "        for question_augmented in question_augmenteds:\n",
    "            qlens.append(len(question_augmented.split()))\n",
    "        qlens = np.array(qlens)\n",
    "        print('Mean # Tokens:', int(round(1.2 * qlens.mean())))\n",
    "        print('# Examples:', len(qlens))\n",
    "\n",
    "        qlens.sort()\n",
    "        print('90 %-ile # Tokens:', int(round(1.2 * qlens[int(.9 * qlens.size)])))\n",
    "        print('99 %-ile # Tokens:', int(round(1.2 * qlens[int(.99 * qlens.size)])))\n",
    "\n",
    "        input_name = 'q'\n",
    "        if use_subq:\n",
    "            input_name += '-predsubqs'\n",
    "        if use_suba:\n",
    "            input_name += '-predsubas'\n",
    "\n",
    "        save_dir = f'{args.subqs_dir}.{input_name}.num_subas={args.num_subas}'\n",
    "        print(f'Saving to {save_dir}...')\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        for split in args.splits:\n",
    "            with open(f'{save_dir}/{split}.suba1={suba_rank[0]}.suba2={suba_rank[1]}.json', 'w') as f:\n",
    "                json.dump(data_copy[split], f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate (subset of) comparison-only Q's\n",
    "if not args.subsample_data:\n",
    "    print('Done! (Not making subsamples of data)')\n",
    "    exit()\n",
    "\n",
    "data_copy = deepcopy(data)\n",
    "num_comp_qs = {}\n",
    "comp_data = {}\n",
    "for split in args.splits:\n",
    "    num_comp_qs[split] = 0\n",
    "    comp_data[split] = {'data': [], 'version': data_copy[split]['version']}\n",
    "    for article in data_copy[split]['data']:\n",
    "        is_comp_paragraph = False\n",
    "        for paragraph in article['paragraphs']:\n",
    "            if paragraph.get('_id') in subas_c_qids:\n",
    "                is_comp_paragraph = True\n",
    "                break\n",
    "        if is_comp_paragraph:\n",
    "            comp_data[split]['data'].append(article)\n",
    "            num_comp_qs[split] += 1\n",
    "print('num_comp_qs:', num_comp_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = f'{DATA_DIR}/hotpot-all'\n",
    "comp_save_dir = f'{save_dir}.comparison-only.num-train=' + str(num_comp_qs['train'])\n",
    "print(f'Saving to {comp_save_dir}...')\n",
    "os.makedirs(comp_save_dir, exist_ok=False)\n",
    "for split in args.splits:\n",
    "    with open(f'{comp_save_dir}/{split}.json', 'w') as f:\n",
    "        json.dump((comp_data if split == 'train' else data_copy)[split], f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_qidxs_train = list(range(num_comp_qs['train']))\n",
    "random.Random(42).shuffle(shuffled_qidxs_train)\n",
    "train_data_frac = .015625\n",
    "\n",
    "train_qidxs_subset = set(shuffled_qidxs_train[:round(len(shuffled_qidxs_train) * train_data_frac)])\n",
    "\n",
    "num_comp_qs_filtered = {}\n",
    "comp_data = {}\n",
    "for split in ['train']:\n",
    "    num_comp_qs_filtered[split] = 0\n",
    "    comp_data[split] = {'data': [], 'version': data_copy[split]['version']}\n",
    "    for article in data_copy[split]['data']:\n",
    "        is_comp_paragraph = False\n",
    "        for paragraph in article['paragraphs']:\n",
    "            if paragraph.get('_id') in subas_c_qids:\n",
    "                is_comp_paragraph = True\n",
    "                break\n",
    "        if is_comp_paragraph:\n",
    "            if num_comp_qs_filtered[split] in train_qidxs_subset:\n",
    "                comp_data[split]['data'].append(article)\n",
    "            num_comp_qs_filtered[split] += 1\n",
    "print('Train Comp Qs remaining:', len(comp_data['train']['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_save_dir = f'{save_dir}.comparison-only.num-train=' + str(len(comp_data['train']['data']))\n",
    "print(f'Saving to {comp_save_dir}...')\n",
    "os.makedirs(comp_save_dir, exist_ok=False)\n",
    "for split in args.splits:\n",
    "    with open(f'{comp_save_dir}/{split}.json', 'w') as f:\n",
    "        json.dump((comp_data if split == 'train' else data_copy)[split], f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
